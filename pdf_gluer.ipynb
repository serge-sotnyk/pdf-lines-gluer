{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from typing import List, Dict\n",
    "import string\n",
    "import random\n",
    "import pdf_lines_gluer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['corpus\\\\1005058.txt', 'corpus\\\\1005395.txt', 'corpus\\\\104888.txt', 'corpus\\\\105529.txt', 'corpus\\\\200850.txt', 'corpus\\\\200851.txt', 'corpus\\\\300125.txt', 'corpus\\\\300138.txt', 'corpus\\\\500150.txt', 'corpus\\\\500486.txt', 'corpus\\\\601777.txt', 'corpus\\\\601779.txt']\n",
      "File 'corpus\\1005058.txt' is annotated, add to collection.\n",
      "File 'corpus\\1005395.txt' is annotated, add to collection.\n",
      "File 'corpus\\104888.txt' is not annotated, skipped.\n",
      "File 'corpus\\105529.txt' is not annotated, skipped.\n",
      "File 'corpus\\200850.txt' is annotated, add to collection.\n",
      "File 'corpus\\200851.txt' is not annotated, skipped.\n",
      "File 'corpus\\300125.txt' is not annotated, skipped.\n",
      "File 'corpus\\300138.txt' is not annotated, skipped.\n",
      "File 'corpus\\500150.txt' is not annotated, skipped.\n",
      "File 'corpus\\500486.txt' is not annotated, skipped.\n",
      "File 'corpus\\601777.txt' is not annotated, skipped.\n",
      "File 'corpus\\601779.txt' is not annotated, skipped.\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def load_texts():\n",
    "    f_names = [join('corpus', f) for f in listdir('corpus') if isfile(join('corpus', f))]\n",
    "    f_names = [f for f in f_names if f.lower().endswith('.txt')]\n",
    "    print(f_names)\n",
    "    for fn in f_names:\n",
    "        with open(fn, 'rt', encoding='utf-8', errors='replace') as f:\n",
    "            text = f.read()\n",
    "            if not text[0] in {'+', '*'}:\n",
    "                print(f\"File '{fn}' is not annotated, skipped.\")\n",
    "                continue\n",
    "            print(f\"File '{fn}' is annotated, add to collection.\")\n",
    "            yield text\n",
    "            \n",
    "raw_corpus = list(load_texts())\n",
    "print(len(raw_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = pdf_lines_gluer._featurize_text_with_annotation(raw_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'this_len': 12, 'mean_len': 75.0, 'prev_len': 0, 'prev_glued': 0, 'first_chars': 'Aa', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': ' '}, {'this_len': 97, 'mean_len': 79.33333333333333, 'prev_len': 0, 'prev_glued': 0, 'first_chars': 'Aa', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': ' '}, {'this_len': 104, 'mean_len': 82.71428571428571, 'prev_len': 11, 'prev_glued': 0, 'first_chars': 'aa', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': '.'}, {'this_len': 62, 'mean_len': 79.875, 'prev_len': 11, 'prev_glued': 1, 'first_chars': 'a-', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': '.'}, {'this_len': 100, 'mean_len': 81.88888888888889, 'prev_len': 11, 'prev_glued': 2, 'first_chars': 'Aa', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': '.'}, {'this_len': 101, 'mean_len': 84.2, 'prev_len': 11, 'prev_glued': 0, 'first_chars': 'aa', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': '.'}, {'this_len': 103, 'mean_len': 92.7, 'prev_len': 11, 'prev_glued': 1, 'first_chars': 'aa', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': '.'}, {'this_len': 60, 'mean_len': 93.1, 'prev_len': 11, 'prev_glued': 2, 'first_chars': 'aa', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': '.'}, {'this_len': 98, 'mean_len': 92.5, 'prev_len': 11, 'prev_glued': 3, 'first_chars': 'Aa', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': '.'}, {'this_len': 105, 'mean_len': 97.1, 'prev_len': 11, 'prev_glued': 0, 'first_chars': 'AA', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': '.'}]\n"
     ]
    }
   ],
   "source": [
    "print(x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2300\n",
      "Positive samples: 1611\n"
     ]
    }
   ],
   "source": [
    "xx, yy = [], []\n",
    "for raw_text in raw_corpus:\n",
    "    x, y = pdf_lines_gluer._featurize_text_with_annotation(raw_text)\n",
    "    xx+=x\n",
    "    yy+=y\n",
    "print(f\"Total samples: {len(yy)}\")\n",
    "print(f\"Positive samples: {sum(y for y in yy if y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1974)\n",
    "combined = list(zip(xx, yy))\n",
    "random.shuffle(combined)\n",
    "xx[:], yy[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = DictVectorizer(sparse=False)\n",
    "v.fit(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.\n",
      "  39.1  1.  30.   0.   1.  36. ]]\n"
     ]
    }
   ],
   "source": [
    "xx_features = v.transform(xx)\n",
    "print(xx_features[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(xx_features, yy, test_size=0.3, random_state=1974)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.79      0.86      0.82       207\n",
      "        True       0.94      0.90      0.92       483\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       690\n",
      "   macro avg       0.86      0.88      0.87       690\n",
      "weighted avg       0.89      0.89      0.89       690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#clf = RandomForestClassifier(random_state=1974)\n",
    "clf = LogisticRegression(random_state=1974, solver='liblinear', max_iter=2000,\n",
    "                         # class_weight='balanced'\n",
    "                        )\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The rapid expansion of wireless services such as cellular voice, PCS\n",
    "(Personal Communications Services), mobile data and wireless LANs\n",
    "in recent years is an indication that signicant value is placed on accessibility\n",
    "and portability as key features of telecommunication (Salkintzis and Mathiopoulos (Guest Ed.), 2000).\n",
    "devices have maximum utility when they can be used any-\n",
    "where at anytime\". One of the greatest limitations to that goal, how-\n",
    "ever, is nite power supplies. Since batteries provide limited power, a\n",
    "general constraint of wireless communication is the short continuous\n",
    "operation time of mobile terminals. Therefore, power management is\n",
    "y Corresponding Author: Dr. Krishna Sivalingam. Part of the research was\n",
    "supported by Air Force Oce of Scientic Research grants F-49620-97-1-\n",
    "0471 and F-49620-99-1-0125; by Telcordia Technologies and by Intel. Part of\n",
    "the work was done while the rst author was at Washington State Univer-\n",
    "sity. The authors' can be reached at cej@bbn.com, krishna@eecs.wsu.edu,\n",
    "pagrawal@research.telcordia.com, jcchen@research.telcordia.com\n",
    "c\n",
    "2001 Kluwer Academic Publishers. Printed in the Netherlands.\n",
    "Jones, Sivalingam, Agrawal and Chen\n",
    "one of the most challenging problems in wireless communication, and\n",
    "recent research has addressed this topic (Bambos, 1998). Examples include\n",
    "a collection of papers available in (Zorzi (Guest Ed.), 1998) and\n",
    "a recent conference tutorial (Srivastava, 2000), both devoted to energy\n",
    "ecient design of wireless networks.\n",
    "Studies show that the signicant consumers of power in a typical\n",
    "laptop are the microprocessor (CPU), liquid crystal display (LCD),\n",
    "hard disk, system memory (DRAM), keyboard/mouse, CDROM drive,\n",
    "oppy drive, I/O subsystem, and the wireless network interface card\n",
    "(Udani and Smith, 1996, Stemm and Katz, 1997). A typical example\n",
    "from a Toshiba 410 CDT mobile computer demonstrates that nearly\n",
    "36% of power consumed is by the display, 21% by the CPU/memory,\n",
    "18% by the wireless interface, and 18% by the hard drive. Consequently,\n",
    "energy conservation has been largely considered in the hardware design\n",
    "of the mobile terminal (Chandrakasan and Brodersen, 1995) and in\n",
    "components such as CPU, disks, displays, etc. Signicant additional\n",
    "power savings may result by incorporating low-power strategies into\n",
    "the design of network protocols used for data communication. This\n",
    "paper addresses the incorporation of energy conservation at all layers\n",
    "of the protocol stack for wireless networks.\n",
    "The remainder of this paper is organized as follows. Section 2 introduces\n",
    "the network architectures and wireless protocol stack considered\n",
    "in this paper. Low-power design within the physical layer is brie\n",
    "y\n",
    "discussed in Section 2.3. Sources of power consumption within mobile\n",
    "terminals and general guidelines for reducing the power consumed are\n",
    "presented in Section 3. Section 4 describes work dealing with energy\n",
    "ecient protocols within the MAC layer of wireless networks, and\n",
    "power conserving protocols within the LLC layer are addressed in Section\n",
    "5. Section 6 discusses power aware protocols within the network\n",
    "layer. Opportunities for saving battery power within the transport\n",
    "layer are discussed in Section 7. Section 8 presents techniques at the\n",
    "OS/middleware and application layers for energy ecient operation.\n",
    "Finally, Section 9 summarizes and concludes the paper.\n",
    "2. Background\n",
    "This section describes the wireless network architectures considered in\n",
    "this paper. Also, a discussion of the wireless protocol stack is included\n",
    "along with a brief description of each individual protocol layer. The\n",
    "physical layer is further discussed. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rapid expansion of wireless services such as cellular voice, PCS (Personal Communications Services), mobile data and wireless LANs in recent years is an indication that signicant value is placed on accessibility and portability as key features of telecommunication (Salkintzis and Mathiopoulos (Guest Ed.), 2000). devices have maximum utility when they can be used anywhere at anytime\". One of the greatest limitations to that goal, however, is nite power supplies. Since batteries provide limited power, a general constraint of wireless communication is the short continuous operation time of mobile terminals. Therefore, power management is y Corresponding Author: Dr. Krishna Sivalingam. Part of the research was supported by Air Force Oce of Scientic Research grants F-49620-97-10471 and F-49620-99-1-0125; by Telcordia Technologies and by Intel. Part of the work was done while the rst author was at Washington State University. The authors' can be reached at cej@bbn.com, krishna@eecs.wsu.edu, pagrawal@research.telcordia.com, jcchen@research.telcordia.com c 2001 Kluwer Academic Publishers. Printed in the Netherlands.\n",
      "Jones, Sivalingam, Agrawal and Chen one of the most challenging problems in wireless communication, and recent research has addressed this topic (Bambos, 1998). Examples include a collection of papers available in (Zorzi (Guest Ed.), 1998) and a recent conference tutorial (Srivastava, 2000), both devoted to energy ecient design of wireless networks.\n",
      "Studies show that the signicant consumers of power in a typical laptop are the microprocessor (CPU), liquid crystal display (LCD), hard disk, system memory (DRAM), keyboard/mouse, CDROM drive, oppy drive, I/O subsystem, and the wireless network interface card (Udani and Smith, 1996, Stemm and Katz, 1997). A typical example from a Toshiba 410 CDT mobile computer demonstrates that nearly 36% of power consumed is by the display, 21% by the CPU/memory, 18% by the wireless interface, and 18% by the hard drive. Consequently, energy conservation has been largely considered in the hardware design of the mobile terminal (Chandrakasan and Brodersen, 1995) and in components such as CPU, disks, displays, etc. Signicant additional power savings may result by incorporating low-power strategies into the design of network protocols used for data communication. This paper addresses the incorporation of energy conservation at all layers of the protocol stack for wireless networks.\n",
      "The remainder of this paper is organized as follows. Section 2 introduces the network architectures and wireless protocol stack considered in this paper. Low-power design within the physical layer is brie\n",
      "y discussed in Section 2.3. Sources of power consumption within mobile terminals and general guidelines for reducing the power consumed are presented in Section 3. Section 4 describes work dealing with energy ecient protocols within the MAC layer of wireless networks, and power conserving protocols within the LLC layer are addressed in Section\n",
      "5. Section 6 discusses power aware protocols within the network layer. Opportunities for saving battery power within the transport layer are discussed in Section 7. Section 8 presents techniques at the OS/middleware and application layers for energy ecient operation.\n",
      "Finally, Section 9 summarizes and concludes the paper.\n",
      "2. Background\n",
      "This section describes the wireless network architectures considered in this paper. Also, a discussion of the wireless protocol stack is included along with a brief description of each individual protocol layer. The physical layer is further discussed.\n"
     ]
    }
   ],
   "source": [
    "corrected = pdf_lines_gluer._preprocess_pdf(text, clf, v)\n",
    "print(corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_text = \"\"\"Метод опорных векторов предназначен для решения задач клас-\n",
    "сификации путем поиска хороших решающих границ (рис. 1.10), \n",
    "разделяющих два набора точек, принадлежащих разным катего-\n",
    "риям. Решающей границей может быть линия или поверхность, \n",
    "разделяющая выборку обучающих данных на пространства, при-\n",
    "надлежащие двум категориям. Для классификации новых точек \n",
    "достаточно только проверить, по какую сторону от границы они \n",
    "находятся.\n",
    "Поиск таких границ метод опорных векторов осуществляет в два \n",
    "этапа:\n",
    "1. Данные отображаются в новое пространство более высокой \n",
    "размерности, где граница может быть представлена как гипер-\n",
    "плоскость (если данные были двумерными, как на рис. 1.10, \n",
    "гиперплоскость вырождается в линию).\n",
    "2. Хорошая решающая граница (разделяющая гиперплоскость) вычисляется\n",
    "путем максимизации расстояния от гиперплоскости до ближайших точек \n",
    "каждого класса, этот этап называют максимизацией зазора. Это позволяет \n",
    "обобщить классификацию новых образцов, не принадлежащих обучающему \n",
    "набору данных.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метод опорных векторов предназначен для решения задач классификации путем поиска хороших решающих границ (рис. 1.10), разделяющих два набора точек, принадлежащих разным категориям. Решающей границей может быть линия или поверхность, разделяющая выборку обучающих данных на пространства, принадлежащие двум категориям. Для классификации новых точек достаточно только проверить, по какую сторону от границы они находятся.\n",
      "Поиск таких границ метод опорных векторов осуществляет в два этапа:\n",
      "1. Данные отображаются в новое пространство более высокой размерности, где граница может быть представлена как гиперплоскость (если данные были двумерными, как на рис. 1.10, гиперплоскость вырождается в линию).\n",
      "2. Хорошая решающая граница (разделяющая гиперплоскость) вычисляется путем максимизации расстояния от гиперплоскости до ближайших точек каждого класса, этот этап называют максимизацией зазора. Это позволяет обобщить классификацию новых образцов, не принадлежащих обучающему набору данных.\n"
     ]
    }
   ],
   "source": [
    "corrected = pdf_lines_gluer._preprocess_pdf(ru_text, clf, v)\n",
    "print(corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check minimal properties set to save vectorizer and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=False)\n"
     ]
    }
   ],
   "source": [
    "print(repr(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first_chars= ', 'first_chars=\"a', \"first_chars=' \", \"first_chars='A\", 'first_chars=(0', 'first_chars=(A', 'first_chars=(a', 'first_chars=)]', 'first_chars=, ', 'first_chars=. ', 'first_chars=0', 'first_chars=0 ', 'first_chars=0,', 'first_chars=0.', 'first_chars=00', 'first_chars=0:', 'first_chars=0\\\\', 'first_chars=@', 'first_chars=A', 'first_chars=A ', 'first_chars=A,', 'first_chars=A-', 'first_chars=A.', 'first_chars=A0', 'first_chars=A=', 'first_chars=AA', 'first_chars=Aa', 'first_chars=[0', 'first_chars=[A', 'first_chars=[a', 'first_chars=\\\\A', 'first_chars=a ', 'first_chars=a(', 'first_chars=a-', 'first_chars=a.', 'first_chars=a0', 'first_chars=aA', 'first_chars=a[', 'first_chars=aa', 'isalpha', 'isdigit', 'islower', 'mean_len', 'prev_glued', 'prev_len', 'punct= ', 'punct=.', 'this_len']\n",
      "{'first_chars= ': 0, 'first_chars=\"a': 1, \"first_chars=' \": 2, \"first_chars='A\": 3, 'first_chars=(0': 4, 'first_chars=(A': 5, 'first_chars=(a': 6, 'first_chars=)]': 7, 'first_chars=, ': 8, 'first_chars=. ': 9, 'first_chars=0': 10, 'first_chars=0 ': 11, 'first_chars=0,': 12, 'first_chars=0.': 13, 'first_chars=00': 14, 'first_chars=0:': 15, 'first_chars=0\\\\': 16, 'first_chars=@': 17, 'first_chars=A': 18, 'first_chars=A ': 19, 'first_chars=A,': 20, 'first_chars=A-': 21, 'first_chars=A.': 22, 'first_chars=A0': 23, 'first_chars=A=': 24, 'first_chars=AA': 25, 'first_chars=Aa': 26, 'first_chars=[0': 27, 'first_chars=[A': 28, 'first_chars=[a': 29, 'first_chars=\\\\A': 30, 'first_chars=a ': 31, 'first_chars=a(': 32, 'first_chars=a-': 33, 'first_chars=a.': 34, 'first_chars=a0': 35, 'first_chars=aA': 36, 'first_chars=a[': 37, 'first_chars=aa': 38, 'isalpha': 39, 'isdigit': 40, 'islower': 41, 'mean_len': 42, 'prev_glued': 43, 'prev_len': 44, 'punct= ': 45, 'punct=.': 46, 'this_len': 47}\n"
     ]
    }
   ],
   "source": [
    "print(v.feature_names_)\n",
    "print(v.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv = DictVectorizer()\n",
    "vv.feature_names_ = v.feature_names_\n",
    "vv.vocabulary_ = v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[-1.92428293, -0.57402104,  0.2315162 ,  0.22349134, -0.21893115,\n",
      "         0.40302487,  1.06171059, -0.3000657 , -0.80354834, -0.59474902,\n",
      "         0.54936339, -0.95955689,  0.65555019, -1.90063708,  0.72334064,\n",
      "        -0.17199479,  0.15464983,  0.31376216, -0.66997325, -1.18498478,\n",
      "        -0.270456  ,  0.        , -1.5816989 , -0.43472727,  0.49956562,\n",
      "        -0.46305976, -1.72074182,  0.27111279,  0.22436558,  0.        ,\n",
      "         1.32364416,  0.04193252, -0.14462101,  1.64987425,  0.89867317,\n",
      "         0.50371394,  0.2192005 , -0.50426949,  2.57259903,  0.        ,\n",
      "         0.        ,  0.        ,  0.02742372,  0.07301828,  0.03715351,\n",
      "        -0.49300996, -1.40821851,  0.01243852]])\n",
      "array([False,  True])\n",
      "[-1.90122847]\n"
     ]
    }
   ],
   "source": [
    "print(repr(clf.coef_))\n",
    "print(repr(clf.classes_))\n",
    "print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = LogisticRegression()\n",
    "clf2.coef_ = clf.coef_\n",
    "clf2.classes_ = clf.classes_\n",
    "clf2.intercept_ = clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метод опорных векторов предназначен для решения задач классификации путем поиска хороших решающих границ (рис. 1.10), разделяющих два набора точек, принадлежащих разным категориям. Решающей границей может быть линия или поверхность, разделяющая выборку обучающих данных на пространства, принадлежащие двум категориям. Для классификации новых точек достаточно только проверить, по какую сторону от границы они находятся.\n",
      "Поиск таких границ метод опорных векторов осуществляет в два этапа:\n",
      "1. Данные отображаются в новое пространство более высокой размерности, где граница может быть представлена как гиперплоскость (если данные были двумерными, как на рис. 1.10, гиперплоскость вырождается в линию).\n",
      "2. Хорошая решающая граница (разделяющая гиперплоскость) вычисляется путем максимизации расстояния от гиперплоскости до ближайших точек каждого класса, этот этап называют максимизацией зазора. Это позволяет обобщить классификацию новых образцов, не принадлежащих обучающему набору данных.\n"
     ]
    }
   ],
   "source": [
    "corrected = pdf_lines_gluer._preprocess_pdf(ru_text, clf2, vv)\n",
    "print(corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01549817 0.98450183]]\n",
      "[[0.01549817 0.98450183]]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict_proba([x_test[0]]))\n",
    "print(clf2.predict_proba([x_test[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize as code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "serialized_as_code = f\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from numpy import array\n",
    "\n",
    "_clf = LogisticRegression()\n",
    "_clf.coef_ = {repr(clf.coef_)}\n",
    "_clf.classes_ = {repr(clf.classes_)}\n",
    "_clf.intercept_ = {clf.intercept_}\n",
    "\n",
    "_v = DictVectorizer()\n",
    "_v.feature_names_ = {v.feature_names_}\n",
    "_v.vocabulary_ = {v.vocabulary_}\n",
    "\n",
    "def preprocess_pdf(text: str) -> str:\n",
    "    return _preprocess_pdf(text, _clf, _v)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "serialized_as_code = f\"\\n# This code was automatically generated at {datetime.datetime.now()}\\n\"+\\\n",
    "    serialized_as_code+\\\n",
    "    \"# end of automatically generated code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# This code was automatically generated at 2019-05-17 17:56:24.195381\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "from numpy import array\n",
      "\n",
      "_clf = LogisticRegression()\n",
      "_clf.coef_ = array([[-1.92428293, -0.57402104,  0.2315162 ,  0.22349134, -0.21893115,\n",
      "         0.40302487,  1.06171059, -0.3000657 , -0.80354834, -0.59474902,\n",
      "         0.54936339, -0.95955689,  0.65555019, -1.90063708,  0.72334064,\n",
      "        -0.17199479,  0.15464983,  0.31376216, -0.66997325, -1.18498478,\n",
      "        -0.270456  ,  0.        , -1.5816989 , -0.43472727,  0.49956562,\n",
      "        -0.46305976, -1.72074182,  0.27111279,  0.22436558,  0.        ,\n",
      "         1.32364416,  0.04193252, -0.14462101,  1.64987425,  0.89867317,\n",
      "         0.50371394,  0.2192005 , -0.50426949,  2.57259903,  0.        ,\n",
      "         0.        ,  0.        ,  0.02742372,  0.07301828,  0.03715351,\n",
      "        -0.49300996, -1.40821851,  0.01243852]])\n",
      "_clf.classes_ = array([False,  True])\n",
      "_clf.intercept_ = [-1.90122847]\n",
      "\n",
      "_v = DictVectorizer()\n",
      "_v.feature_names_ = ['first_chars= ', 'first_chars=\"a', \"first_chars=' \", \"first_chars='A\", 'first_chars=(0', 'first_chars=(A', 'first_chars=(a', 'first_chars=)]', 'first_chars=, ', 'first_chars=. ', 'first_chars=0', 'first_chars=0 ', 'first_chars=0,', 'first_chars=0.', 'first_chars=00', 'first_chars=0:', 'first_chars=0\\\\', 'first_chars=@', 'first_chars=A', 'first_chars=A ', 'first_chars=A,', 'first_chars=A-', 'first_chars=A.', 'first_chars=A0', 'first_chars=A=', 'first_chars=AA', 'first_chars=Aa', 'first_chars=[0', 'first_chars=[A', 'first_chars=[a', 'first_chars=\\\\A', 'first_chars=a ', 'first_chars=a(', 'first_chars=a-', 'first_chars=a.', 'first_chars=a0', 'first_chars=aA', 'first_chars=a[', 'first_chars=aa', 'isalpha', 'isdigit', 'islower', 'mean_len', 'prev_glued', 'prev_len', 'punct= ', 'punct=.', 'this_len']\n",
      "_v.vocabulary_ = {'first_chars= ': 0, 'first_chars=\"a': 1, \"first_chars=' \": 2, \"first_chars='A\": 3, 'first_chars=(0': 4, 'first_chars=(A': 5, 'first_chars=(a': 6, 'first_chars=)]': 7, 'first_chars=, ': 8, 'first_chars=. ': 9, 'first_chars=0': 10, 'first_chars=0 ': 11, 'first_chars=0,': 12, 'first_chars=0.': 13, 'first_chars=00': 14, 'first_chars=0:': 15, 'first_chars=0\\\\': 16, 'first_chars=@': 17, 'first_chars=A': 18, 'first_chars=A ': 19, 'first_chars=A,': 20, 'first_chars=A-': 21, 'first_chars=A.': 22, 'first_chars=A0': 23, 'first_chars=A=': 24, 'first_chars=AA': 25, 'first_chars=Aa': 26, 'first_chars=[0': 27, 'first_chars=[A': 28, 'first_chars=[a': 29, 'first_chars=\\\\A': 30, 'first_chars=a ': 31, 'first_chars=a(': 32, 'first_chars=a-': 33, 'first_chars=a.': 34, 'first_chars=a0': 35, 'first_chars=aA': 36, 'first_chars=a[': 37, 'first_chars=aa': 38, 'isalpha': 39, 'isdigit': 40, 'islower': 41, 'mean_len': 42, 'prev_glued': 43, 'prev_len': 44, 'punct= ': 45, 'punct=.': 46, 'this_len': 47}\n",
      "\n",
      "def preprocess_pdf(text: str) -> str:\n",
      "    return _preprocess_pdf(text, _clf, _v)\n",
      "\n",
      "# end of automatically generated code\n"
     ]
    }
   ],
   "source": [
    "print(serialized_as_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pdf_lines_gluer.py', 'r', encoding='utf-8') as file:\n",
    "    template = file.read()\n",
    "\n",
    "generated_code = template.replace('# inject code here #', serialized_as_code)\n",
    "\n",
    "with open('pdf_preprocessor.py', 'wt', encoding='utf-8') as file:\n",
    "    file.write(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import string\n",
      "from typing import List, Dict\n",
      "\n",
      "# This code was automatically generated at 2019-05-17 17:56:24.195381\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "from numpy import array\n",
      "\n",
      "_clf = LogisticRegression()\n",
      "_clf.coef_ = array([[-1.92428293, -0.57402104,  0.2315162 ,  0.22349134, -0.21893115,\n",
      "         0.40302487,  1.06171059, -0.3000657 , -0.80354834, -0.59474902,\n",
      "         0.54936339, -0.95955689,  0.65555019, -1.90063708,  0.72334064,\n",
      "        -0.17199479,  0.15464983,  0.31376216, -0.66997325, -1.18498478,\n",
      "        -0.270456  ,  0.        , -1.5816989 , -0.43472727,  0.49956562,\n",
      "        -0.46305976, -1.72074182,  0.27111279,  0.22436558,  0.        ,\n",
      "         1.32364416,  0.04193252, -0.14462101,  1.64987425,  0.89867317,\n",
      "         0.50371394,  0.2192005 , -0.50426949,  2.57259903,  0.        ,\n",
      "         0.        ,  0.        ,  0.02742372,  0.07301828,  0.03715351,\n",
      "        -0.49300996, -1.40821851,  0.01243852]])\n",
      "_clf.classes_ = array([False,  True])\n",
      "_clf.intercept_ = [-1.90122847]\n",
      "\n",
      "_v = DictVectorizer()\n",
      "_v.feature_names_ = ['first_chars= ', 'first_chars=\"a', \"first_chars=' \", \"first_chars='A\", 'first_chars=(0', 'first_chars=(A', 'first_chars=(a', 'first_chars=)]', 'first_chars=, ', 'first_chars=. ', 'first_chars=0', 'first_chars=0 ', 'first_chars=0,', 'first_chars=0.', 'first_chars=00', 'first_chars=0:', 'first_chars=0\\\\', 'first_chars=@', 'first_chars=A', 'first_chars=A ', 'first_chars=A,', 'first_chars=A-', 'first_chars=A.', 'first_chars=A0', 'first_chars=A=', 'first_chars=AA', 'first_chars=Aa', 'first_chars=[0', 'first_chars=[A', 'first_chars=[a', 'first_chars=\\\\A', 'first_chars=a ', 'first_chars=a(', 'first_chars=a-', 'first_chars=a.', 'first_chars=a0', 'first_chars=aA', 'first_chars=a[', 'first_chars=aa', 'isalpha', 'isdigit', 'islower', 'mean_len', 'prev_glued', 'prev_len', 'punct= ', 'punct=.', 'this_len']\n",
      "_v.vocabulary_ = {'first_chars= ': 0, 'first_chars=\"a': 1, \"first_chars=' \": 2, \"first_chars='A\": 3, 'first_chars=(0': 4, 'first_chars=(A': 5, 'first_chars=(a': 6, 'first_chars=)]': 7, 'first_chars=, ': 8, 'first_chars=. ': 9, 'first_chars=0': 10, 'first_chars=0 ': 11, 'first_chars=0,': 12, 'first_chars=0.': 13, 'first_chars=00': 14, 'first_chars=0:': 15, 'first_chars=0\\\\': 16, 'first_chars=@': 17, 'first_chars=A': 18, 'first_chars=A ': 19, 'first_chars=A,': 20, 'first_chars=A-': 21, 'first_chars=A.': 22, 'first_chars=A0': 23, 'first_chars=A=': 24, 'first_chars=AA': 25, 'first_chars=Aa': 26, 'first_chars=[0': 27, 'first_chars=[A': 28, 'first_chars=[a': 29, 'first_chars=\\\\A': 30, 'first_chars=a ': 31, 'first_chars=a(': 32, 'first_chars=a-': 33, 'first_chars=a.': 34, 'first_chars=a0': 35, 'first_chars=aA': 36, 'first_chars=a[': 37, 'first_chars=aa': 38, 'isalpha': 39, 'isdigit': 40, 'islower': 41, 'mean_len': 42, 'prev_glued': 43, 'prev_len': 44, 'punct= ': 45, 'punct=.': 46, 'this_len': 47}\n",
      "\n",
      "def preprocess_pdf(text: str) -> str:\n",
      "    return _preprocess_pdf(text, _clf, _v)\n",
      "\n",
      "# end of automatically generated code\n",
      "\n",
      "\n",
      "def _mean_in_window(lines, i) -> float:\n",
      "    start = max(i - 5, 0)\n",
      "    finish = min(i + 5, len(lines) - 1)\n",
      "    sm, count = 0, 0\n",
      "    for n in range(start, finish):\n",
      "        sm += len(lines[n]) - 1  # minus one-char prefix\n",
      "        count += 1\n",
      "    return sm / max(count, 1)\n",
      "\n",
      "\n",
      "def _last_char(line: str) -> str:\n",
      "    return ' ' if len(line) < 1 else line[-1]\n",
      "\n",
      "\n",
      "def _last_char_features(l_char: str) -> Dict[str, object]:\n",
      "    res = {\n",
      "        'isalpha': l_char.isalpha(),\n",
      "        'isdigit': l_char.isdigit(),\n",
      "        'islower': l_char.islower(),\n",
      "        'punct': l_char if l_char in string.punctuation else ' ',\n",
      "    }\n",
      "    return res\n",
      "\n",
      "\n",
      "def _first_chars(line: str) -> str:\n",
      "    if len(line) < 1:\n",
      "        chars = ' '\n",
      "    elif len(line) < 2:\n",
      "        chars = line[0]\n",
      "    else:\n",
      "        chars = line[:2]\n",
      "    res = []\n",
      "    for c in chars:\n",
      "        if c.isdigit():\n",
      "            res.append('0')\n",
      "        elif c.isalpha():\n",
      "            res.append('a' if c.islower() else 'A')\n",
      "        else:\n",
      "            res.append(c)\n",
      "    return ''.join(res)\n",
      "\n",
      "\n",
      "def _line_to_features(line: str, i: int, lines: List[str], y: List[bool]) -> Dict[str, object]:\n",
      "    features = {}\n",
      "    this_len = len(line)\n",
      "    mean_len = _mean_in_window(lines, i)\n",
      "    if i > 1:\n",
      "        prev_len = len(lines[-1]) - 1\n",
      "        l_char = _last_char(lines[-1])\n",
      "    else:\n",
      "        prev_len = 0\n",
      "        l_char = ' '\n",
      "    prev_glued = 0  # How many lines before was glued\n",
      "    for p in range(i - 1, max(-1, i - 10), -1):  # Calc only up to ten items in the sequence\n",
      "        if y[p]:\n",
      "            prev_glued += 1\n",
      "        else:\n",
      "            break\n",
      "    features.update(\n",
      "        {\n",
      "            'this_len': this_len,\n",
      "            'mean_len': mean_len,\n",
      "            'prev_len': prev_len,\n",
      "            'prev_glued': prev_glued,\n",
      "            'first_chars': _first_chars(line),\n",
      "        })\n",
      "    features.update(_last_char_features(l_char))\n",
      "    return features\n",
      "\n",
      "\n",
      "def _featurize_text_with_annotation(text: str) -> (List[object], List[bool]):\n",
      "    lines = text.strip().splitlines()\n",
      "    x, y = [], []\n",
      "    for i, line in enumerate(lines):\n",
      "        y.append(line[0] == '+')  # True, if line should be glued with previous\n",
      "        line = line[1:]\n",
      "        x.append(_line_to_features(line, i, lines, y))\n",
      "    return x, y\n",
      "\n",
      "\n",
      "_HYPHEN_CHARS = {\n",
      "    '\\u002D',  # HYPHEN-MINUS\n",
      "    '\\u00AD',  # SOFT HYPHEN\n",
      "    '\\u2010',  # HYPHEN\n",
      "    '\\u2011',  # NON-BREAKING HYPHEN\n",
      "}\n",
      "\n",
      "\n",
      "def _preprocess_pdf(text: str, clf, v) -> str:\n",
      "    lines = [s.strip() for s in text.strip().splitlines()]\n",
      "    y_pred = []\n",
      "    for i, line in enumerate(lines):\n",
      "        x_one_sample = _line_to_features(line, i, lines, y_pred)\n",
      "        x_one_sample_features = v.transform([x_one_sample])\n",
      "        y_one_pred = clf.predict(x_one_sample_features)\n",
      "        y_pred.append(y_one_pred[0])\n",
      "\n",
      "    corrected_acc = []\n",
      "    for i, line in enumerate(lines):\n",
      "        line = line.strip()\n",
      "        if i == 0 or not y_pred[i]:\n",
      "            corrected_acc.append(line)\n",
      "        else:\n",
      "            prev_line = corrected_acc[-1]\n",
      "            if prev_line[-1] in _HYPHEN_CHARS:\n",
      "                corrected_acc[-1] = prev_line[:-1]\n",
      "            else:\n",
      "                corrected_acc[-1] += ' '\n",
      "            corrected_acc[-1] += line\n",
      "\n",
      "    corrected = '\\n'.join(corrected_acc)\n",
      "    return corrected\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(clf.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check generated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_preprocessor import preprocess_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метод опорных векторов предназначен для решения задач классификации путем поиска хороших решающих границ (рис. 1.10), разделяющих два набора точек, принадлежащих разным категориям. Решающей границей может быть линия или поверхность, разделяющая выборку обучающих данных на пространства, принадлежащие двум категориям. Для классификации новых точек достаточно только проверить, по какую сторону от границы они находятся.\n",
      "Поиск таких границ метод опорных векторов осуществляет в два этапа:\n",
      "1. Данные отображаются в новое пространство более высокой размерности, где граница может быть представлена как гиперплоскость (если данные были двумерными, как на рис. 1.10, гиперплоскость вырождается в линию).\n",
      "2. Хорошая решающая граница (разделяющая гиперплоскость) вычисляется путем максимизации расстояния от гиперплоскости до ближайших точек каждого класса, этот этап называют максимизацией зазора. Это позволяет обобщить классификацию новых образцов, не принадлежащих обучающему набору данных.\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_pdf(ru_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rapid expansion of wireless services such as cellular voice, PCS (Personal Communications Services), mobile data and wireless LANs in recent years is an indication that signicant value is placed on accessibility and portability as key features of telecommunication (Salkintzis and Mathiopoulos (Guest Ed.), 2000). devices have maximum utility when they can be used anywhere at anytime\". One of the greatest limitations to that goal, however, is nite power supplies. Since batteries provide limited power, a general constraint of wireless communication is the short continuous operation time of mobile terminals. Therefore, power management is y Corresponding Author: Dr. Krishna Sivalingam. Part of the research was supported by Air Force Oce of Scientic Research grants F-49620-97-10471 and F-49620-99-1-0125; by Telcordia Technologies and by Intel. Part of the work was done while the rst author was at Washington State University. The authors' can be reached at cej@bbn.com, krishna@eecs.wsu.edu, pagrawal@research.telcordia.com, jcchen@research.telcordia.com c 2001 Kluwer Academic Publishers. Printed in the Netherlands.\n",
      "Jones, Sivalingam, Agrawal and Chen one of the most challenging problems in wireless communication, and recent research has addressed this topic (Bambos, 1998). Examples include a collection of papers available in (Zorzi (Guest Ed.), 1998) and a recent conference tutorial (Srivastava, 2000), both devoted to energy ecient design of wireless networks.\n",
      "Studies show that the signicant consumers of power in a typical laptop are the microprocessor (CPU), liquid crystal display (LCD), hard disk, system memory (DRAM), keyboard/mouse, CDROM drive, oppy drive, I/O subsystem, and the wireless network interface card (Udani and Smith, 1996, Stemm and Katz, 1997). A typical example from a Toshiba 410 CDT mobile computer demonstrates that nearly 36% of power consumed is by the display, 21% by the CPU/memory, 18% by the wireless interface, and 18% by the hard drive. Consequently, energy conservation has been largely considered in the hardware design of the mobile terminal (Chandrakasan and Brodersen, 1995) and in components such as CPU, disks, displays, etc. Signicant additional power savings may result by incorporating low-power strategies into the design of network protocols used for data communication. This paper addresses the incorporation of energy conservation at all layers of the protocol stack for wireless networks.\n",
      "The remainder of this paper is organized as follows. Section 2 introduces the network architectures and wireless protocol stack considered in this paper. Low-power design within the physical layer is brie\n",
      "y discussed in Section 2.3. Sources of power consumption within mobile terminals and general guidelines for reducing the power consumed are presented in Section 3. Section 4 describes work dealing with energy ecient protocols within the MAC layer of wireless networks, and power conserving protocols within the LLC layer are addressed in Section\n",
      "5. Section 6 discusses power aware protocols within the network layer. Opportunities for saving battery power within the transport layer are discussed in Section 7. Section 8 presents techniques at the OS/middleware and application layers for energy ecient operation.\n",
      "Finally, Section 9 summarizes and concludes the paper.\n",
      "2. Background\n",
      "This section describes the wireless network architectures considered in this paper. Also, a discussion of the wireless protocol stack is included along with a brief description of each individual protocol layer. The physical layer is further discussed.\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_pdf(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
