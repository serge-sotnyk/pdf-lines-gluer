{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from typing import List, Dict\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['corpus\\\\1005058.txt', 'corpus\\\\1005395.txt', 'corpus\\\\104888.txt', 'corpus\\\\105529.txt', 'corpus\\\\200850.txt', 'corpus\\\\200851.txt', 'corpus\\\\300125.txt', 'corpus\\\\300138.txt', 'corpus\\\\500150.txt', 'corpus\\\\500486.txt', 'corpus\\\\601777.txt', 'corpus\\\\601779.txt']\n",
      "File 'corpus\\1005058.txt' is annotated, add to collection.\n",
      "File 'corpus\\1005395.txt' is annotated, add to collection.\n",
      "File 'corpus\\104888.txt' is not annotated, skipped.\n",
      "File 'corpus\\105529.txt' is not annotated, skipped.\n",
      "File 'corpus\\200850.txt' is annotated, add to collection.\n",
      "File 'corpus\\200851.txt' is not annotated, skipped.\n",
      "File 'corpus\\300125.txt' is not annotated, skipped.\n",
      "File 'corpus\\300138.txt' is not annotated, skipped.\n",
      "File 'corpus\\500150.txt' is not annotated, skipped.\n",
      "File 'corpus\\500486.txt' is not annotated, skipped.\n",
      "File 'corpus\\601777.txt' is not annotated, skipped.\n",
      "File 'corpus\\601779.txt' is not annotated, skipped.\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def load_texts():\n",
    "    f_names = [join('corpus', f) for f in listdir('corpus') if isfile(join('corpus', f))]\n",
    "    f_names = [f for f in f_names if f.lower().endswith('.txt')]\n",
    "    print(f_names)\n",
    "    for fn in f_names:\n",
    "        with open(fn, 'rt', encoding='utf-8', errors='replace') as f:\n",
    "            text = f.read()\n",
    "            if not text[0] in {'+', '*'}:\n",
    "                print(f\"File '{fn}' is not annotated, skipped.\")\n",
    "                continue\n",
    "            print(f\"File '{fn}' is annotated, add to collection.\")\n",
    "            yield text\n",
    "            \n",
    "raw_corpus = list(load_texts())\n",
    "print(len(raw_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_in_window(lines, i)->float:\n",
    "    start = max(i-5, 0)\n",
    "    finish = min(i+5, len(lines)-1)\n",
    "    sm, count = 0, 0\n",
    "    for n in range(start, finish):\n",
    "        sm += len(lines[n])-1  # minus one-char prefix\n",
    "        count += 1\n",
    "    return sm / max(count, 1)\n",
    "\n",
    "def last_char(line: str)->str:    \n",
    "    return ' ' if len(line)<1 else line[-1]\n",
    "    \n",
    "def last_char_features(l_char: str)->Dict[str, object]:\n",
    "    res = {\n",
    "        'isalpha': l_char.isalpha(),\n",
    "        'isdigit': l_char.isdigit(),\n",
    "        'islower': l_char.islower(),\n",
    "        'punct': l_char if l_char in string.punctuation else ' ',\n",
    "    }\n",
    "    return res\n",
    "\n",
    "\n",
    "def first_chars(line: str)->str:    \n",
    "    if len(line)<1:\n",
    "        chars = ' '\n",
    "    elif len(line)<2:\n",
    "        chars = line[0]\n",
    "    else:\n",
    "        chars = line[:2]\n",
    "    res = []\n",
    "    for c in chars:\n",
    "        if c.isdigit():\n",
    "            res.append('0')\n",
    "        elif c.isalpha():\n",
    "            res.append('a' if c.islower() else 'A')\n",
    "        else:\n",
    "            res.append(c)\n",
    "    return ''.join(res)\n",
    "\n",
    "def line_to_features(line: str)->Dict[str, object]:\n",
    "    features = {}\n",
    "    this_len = len(line)\n",
    "    mean_len = mean_in_window(lines, i)\n",
    "    if i>1:\n",
    "        prev_len = len(lines[-1])-1\n",
    "        l_char = last_char(lines[-1])\n",
    "    else:\n",
    "        prev_len = 0\n",
    "        l_char = ' '\n",
    "    prev_glued = 0  # How many lines before was glued\n",
    "    for p in range(i-1, max(0, i-10), -1):  # Calc only up to ten items in the sequence\n",
    "        if y[p]:\n",
    "            prev_glued += 1\n",
    "        else: \n",
    "            break\n",
    "    features.update(\n",
    "        {\n",
    "            'this_len': this_len,\n",
    "            'mean_len': mean_len,\n",
    "            'prev_len': prev_len,\n",
    "            'prev_glued': prev_glued,\n",
    "            'first_chars': first_chars(line),\n",
    "        })\n",
    "    features.update(last_char_features(l_char))\n",
    "    x.append(features)\n",
    "\n",
    "def featurize_text_with_annotation(text: str)->(List[object], List[bool]):\n",
    "    lines = text.strip().splitlines()\n",
    "    total_lines = len(lines)\n",
    "    x, y = [], []\n",
    "    for i, line in enumerate(lines):        \n",
    "        features = {}\n",
    "        y.append(line[0]=='+')  # True, if line should be glued with previous\n",
    "        line = line[1:]\n",
    "        # print(y[-1], line)\n",
    "        this_len = len(line)\n",
    "        mean_len = mean_in_window(lines, i)\n",
    "        if i>1:\n",
    "            prev_len = len(lines[-1])-1\n",
    "            l_char = last_char(lines[-1])\n",
    "        else:\n",
    "            prev_len = 0\n",
    "            l_char = ' '\n",
    "        prev_glued = 0  # How many lines before was glued\n",
    "        for p in range(i-1, max(0, i-10), -1):  # Calc only up to ten items in the sequence\n",
    "            if y[p]:\n",
    "                prev_glued += 1\n",
    "            else: \n",
    "                break\n",
    "        features.update(\n",
    "            {\n",
    "                'this_len': this_len,\n",
    "                'mean_len': mean_len,\n",
    "                'prev_len': prev_len,\n",
    "                'prev_glued': prev_glued,\n",
    "                'first_chars': first_chars(line),\n",
    "            })\n",
    "        features.update(last_char_features(l_char))\n",
    "        x.append(features)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = featurize_text_with_annotation(raw_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'this_len': 12, 'mean_len': 75.0, 'prev_len': 0, 'prev_glued': 0, 'first_chars': 'Aa', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': ' '}, {'this_len': 97, 'mean_len': 79.33333333333333, 'prev_len': 0, 'prev_glued': 0, 'first_chars': 'Aa', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': ' '}, {'this_len': 104, 'mean_len': 82.71428571428571, 'prev_len': 11, 'prev_glued': 0, 'first_chars': 'aa', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': '.'}, {'this_len': 62, 'mean_len': 79.875, 'prev_len': 11, 'prev_glued': 1, 'first_chars': 'a-', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': '.'}, {'this_len': 100, 'mean_len': 81.88888888888889, 'prev_len': 11, 'prev_glued': 2, 'first_chars': 'Aa', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': '.'}, {'this_len': 101, 'mean_len': 84.2, 'prev_len': 11, 'prev_glued': 0, 'first_chars': 'aa', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': '.'}, {'this_len': 103, 'mean_len': 92.7, 'prev_len': 11, 'prev_glued': 1, 'first_chars': 'aa', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': '.'}, {'this_len': 60, 'mean_len': 93.1, 'prev_len': 11, 'prev_glued': 2, 'first_chars': 'aa', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': '.'}, {'this_len': 98, 'mean_len': 92.5, 'prev_len': 11, 'prev_glued': 3, 'first_chars': 'Aa', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': '.'}, {'this_len': 105, 'mean_len': 97.1, 'prev_len': 11, 'prev_glued': 0, 'first_chars': 'AA', 'isalpha': False, 'isdigit': False, 'islower': False, 'punct': '.'}]\n"
     ]
    }
   ],
   "source": [
    "print(x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2300\n",
      "Positive samples: 1611\n"
     ]
    }
   ],
   "source": [
    "xx, yy = [], []\n",
    "for raw_text in raw_corpus:\n",
    "    x, y = featurize_text(raw_text)\n",
    "    xx+=x\n",
    "    yy+=y\n",
    "print(f\"Total samples: {len(yy)}\")\n",
    "print(f\"Positive samples: {sum(y for y in yy if y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1974)\n",
    "combined = list(zip(xx, yy))\n",
    "random.shuffle(combined)\n",
    "xx[:], yy[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = DictVectorizer(sparse=False)\n",
    "v.fit(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.\n",
      "  39.1  1.  30.   0.   1.  36. ]]\n"
     ]
    }
   ],
   "source": [
    "xx_features = v.transform(xx)\n",
    "print(xx_features[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(xx_features, yy, test_size=0.3, random_state=1974)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.79      0.86      0.82       207\n",
      "        True       0.94      0.90      0.92       483\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       690\n",
      "   macro avg       0.86      0.88      0.87       690\n",
      "weighted avg       0.89      0.89      0.89       690\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssotn\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#clf = RandomForestClassifier(random_state=1974)\n",
    "clf = LogisticRegression(random_state=1974)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The rapid expansion of wireless services such as cellular voice, PCS\n",
    "(Personal Communications Services), mobile data and wireless LANs\n",
    "in recent years is an indication that signicant value is placed on accessibility\n",
    "and portability as key features of telecommunication (Salkintzis and Mathiopoulos (Guest Ed.), 2000).\n",
    "devices have maximum utility when they can be used \\any-\n",
    "where at anytime\". One of the greatest limitations to that goal, how-\n",
    "ever, is nite power supplies. Since batteries provide limited power, a\n",
    "general constraint of wireless communication is the short continuous\n",
    "operation time of mobile terminals. Therefore, power management is\n",
    "y Corresponding Author: Dr. Krishna Sivalingam. Part of the research was\n",
    "supported by Air Force Oce of Scientic Research grants F-49620-97-1-\n",
    "0471 and F-49620-99-1-0125; by Telcordia Technologies and by Intel. Part of\n",
    "the work was done while the rst author was at Washington State Univer-\n",
    "sity. The authors' can be reached at cej@bbn.com, krishna@eecs.wsu.edu,\n",
    "pagrawal@research.telcordia.com, jcchen@research.telcordia.com\n",
    "c\n",
    "2001 Kluwer Academic Publishers. Printed in the Netherlands.\n",
    "Jones, Sivalingam, Agrawal and Chen\n",
    "one of the most challenging problems in wireless communication, and\n",
    "recent research has addressed this topic (Bambos, 1998). Examples include\n",
    "a collection of papers available in (Zorzi (Guest Ed.), 1998) and\n",
    "a recent conference tutorial (Srivastava, 2000), both devoted to energy\n",
    "ecient design of wireless networks.\n",
    "Studies show that the signicant consumers of power in a typical\n",
    "laptop are the microprocessor (CPU), liquid crystal display (LCD),\n",
    "hard disk, system memory (DRAM), keyboard/mouse, CDROM drive,\n",
    "oppy drive, I/O subsystem, and the wireless network interface card\n",
    "(Udani and Smith, 1996, Stemm and Katz, 1997). A typical example\n",
    "from a Toshiba 410 CDT mobile computer demonstrates that nearly\n",
    "36% of power consumed is by the display, 21% by the CPU/memory,\n",
    "18% by the wireless interface, and 18% by the hard drive. Consequently,\n",
    "energy conservation has been largely considered in the hardware design\n",
    "of the mobile terminal (Chandrakasan and Brodersen, 1995) and in\n",
    "components such as CPU, disks, displays, etc. Signicant additional\n",
    "power savings may result by incorporating low-power strategies into\n",
    "the design of network protocols used for data communication. This\n",
    "paper addresses the incorporation of energy conservation at all layers\n",
    "of the protocol stack for wireless networks.\n",
    "The remainder of this paper is organized as follows. Section 2 introduces\n",
    "the network architectures and wireless protocol stack considered\n",
    "in this paper. Low-power design within the physical layer is brie\n",
    "y\n",
    "discussed in Section 2.3. Sources of power consumption within mobile\n",
    "terminals and general guidelines for reducing the power consumed are\n",
    "presented in Section 3. Section 4 describes work dealing with energy\n",
    "ecient protocols within the MAC layer of wireless networks, and\n",
    "power conserving protocols within the LLC layer are addressed in Section\n",
    "5. Section 6 discusses power aware protocols within the network\n",
    "layer. Opportunities for saving battery power within the transport\n",
    "layer are discussed in Section 7. Section 8 presents techniques at the\n",
    "OS/middleware and application layers for energy ecient operation.\n",
    "Finally, Section 9 summarizes and concludes the paper.\n",
    "2. Background\n",
    "This section describes the wireless network architectures considered in\n",
    "this paper. Also, a discussion of the wireless protocol stack is included\n",
    "along with a brief description of each individual protocol layer. The\n",
    "physical layer is further discussed. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample, lines = featurize_text(text)\n",
    "x_sample_features = v.transform(x_sample)\n",
    "y_sample = clf.predict(x_sample_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, bool found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-018e7ee54800>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mlines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mcorrected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrected_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, bool found"
     ]
    }
   ],
   "source": [
    "# for test we give first 4 characters from https://en.wikipedia.org/wiki/Hyphen#Unicode\n",
    "hyphen_chars = {\n",
    "                '\\u002D', # HYPHEN-MINUS\n",
    "                '\\u00AD', # SOFT HYPHEN \n",
    "                '\\u2010', # HYPHEN \n",
    "                '\\u2011', # NON-BREAKING HYPHEN\n",
    "               }\n",
    "\n",
    "corrected_acc = []\n",
    "for i, line in enumerate(lines):\n",
    "    if i==0 or not y_sample[i]:\n",
    "        corrected_acc.append(line)\n",
    "    else:\n",
    "        prev_line = lines[i-1]\n",
    "        if prev_line in hyphen_chars:\n",
    "            lines[i-1]=prev_line[:-1]\n",
    "        lines[i-1] += line\n",
    "\n",
    "corrected = ''.join(corrected_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
